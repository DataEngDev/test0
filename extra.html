<!DOCTYPE html> <!-- vim: set foldmethod=marker foldlevel=0 :-->
<html>
<!-- *** Head stuff {{{ -->
<!--[[[cog
import cog
from cogutil import *
include_file_default(classes="medium")
]]]-->
<!--[[[end]]]-->
<head>
    <meta http-equiv="Content-Type" content="text/html;charset=utf-8" />
    <!-- Slide meta data, remove/edit as you see fit -->
    <title>Extra Testing slides</title>
    <meta charset="utf-8" />

    <!-- Slippy core file and dependencies -->
    <script type="text/javascript" src="slippy/jquery.min.js"></script>
    <script type="text/javascript" src="slippy/jquery.history.js"></script>
    <!-- Slippy slides -->
    <script type="text/javascript" src="slippy/slippy.js"></script>
    <script type="text/javascript" src="typogr.min.js"></script>

    <link type="text/css" rel="stylesheet" href="slippy/slippy.css"/>
    <link type="text/css" rel="stylesheet" href="slippy/slippy-pure.css"/>
    <!-- Our styles -->
    <link type="text/css" rel="stylesheet" href="slides.css"/>

    <!-- Highlight, for syntax coloring. -->
    <script type="text/javascript" src="highlight/highlight.pack.js"></script>
    <link rel="stylesheet" href="highlight/vs.css">

    <!-- Ned's slides init code -->
    <script type="text/javascript" src="lineselect.js"></script>

    <link href='http://fonts.googleapis.com/css?family=Open+Sans:400,600,700,800|Permanent+Marker' rel='stylesheet' type='text/css'>

    <script type="text/javascript" src="slides.js"></script>

    <!-- Custom style for this deck -->
    <style type="text/css">
        .slide li.good:before {
            content: "✓   ";    /* U+2713 U+00A0 U+00A0 */
            color: #5b5;
            background-color: none; width: 0; height: 0; top: 0; right: 0;
        }
        .slide li.bad:before {
            content: "✘   ";    /* U+2718 U+00A0 U+00A0 */
            color: #d66;
            background-color: none; width: 0; height: 0; top: 0; right: 0;
        }
        .slide li.unsure:before {
            content: "⁈  ";    /* U+203D U+00A0 U+00A0 */
            color: #66d;
            background-color: none; width: 0; height: 0; top: 0; right: 0;
        }
    </style>

</head>
<!-- }}} -->

<body class="slides">

<!-- *** Slide layouts {{{ -->

<div class="layout" data-name="clean">
    <content></content>
</div>

<div class="layout" data-name="default">
    <content></content>
    <div class="footer">
        <span class="left">bit.ly<span class='punct'>/</span>pytest2</span>
        <span class="right"><span class='punct'>@</span>nedbat</span>
        <hr class="defloat" />
    </div>
</div>

<!-- }}} -->

<!-- *** Introduction {{{ -->

<div class="hidden_slide">
    <h1>Good tests</h1>
    <ul>
        <li>Automated</li>
        <li>Fast</li>
        <li>Reliable</li>
        <li>Informative</li>
        <li>Focused</li>
    </ul>
</div>

<div class="text"><!-- {{{ -->

    <p>Before we look at unittest, let's talk for a minute about what qualities
    make for good tests.</p>

    <p>The whole point of a test is to tell you something about your code: does
    it work?  They are no use to us if they are a burden to run, so we need
    them to be automated, and they need to be fast.</p>

    <p>Tests have to be reliable so that we can use them to guide us.  If tests
    are flaky, or we don't believe their results, then they could give us false
    results.  That would lead us on wild goose chases.  The world is chaotic
    enough, we don't want to add to it by writing tests that pose more
    questions than they answer.  To be reliable, tests must be repeatable, and
    they must be authoritative.  We have to believe what they tell us.</p>

    <p>What they tell us must be informative. Think about your workflow when
    running tests: if the test pass, you go on to write more features, or you
    go outside and enjoy the sunshine.  But what happens when they fail?  Then
    you have to find out what is wrong with your product code so you can fix
    it.  The more the test can tell you about the failure, the more useful they
    are to you.  So we want our tests to be informative.</p>

    <p>We also want them to be focused: the less code tested by each test,
    the more that test tells us when it fails. The broken code must be in the
    code run by the test.  The smaller that chunk of product code, the more
    you've already narrowed down the cause of the failure.  So we want each
    test to test as little code as possible.</p>

    <p>This may seem surprising, often we're looking for code we write to do
    as big a job as possible.  In this case, we want each test to do as little
    as possible, because then when it fails, it will focus us in on a small
    section of product code to fix.</p>

    <!--
    Think of it like this: if broken product code is
    a crime, then the line containing the bug is the perpetrator, and your test
    should narrow down the list of suspects as much as possible.  Don't think
    of your test as testing as little as possible, think of it as exonerating
    as much of your code as possible.
    -->

</div><!-- }}} -->

<!-- }}} -->
<!-- *** Using py.test {{{ -->

<div class="slide section" data-layout="clean">
    <h1>unittest and pytest</h1>
    <h2>Writing tests</h2>
</div>

<div class="slide">
    <h1>unittest</h1>
    <ul>
        <li>In the standard library</li>
        <li>Based on test classes</li>
        <li>Patterned on xUnit</li>
        <li>Wordy, "not Pythonic"</li>
    </ul>
</div>

<div class="text"><!-- {{{ -->

    <p>The Python standard library provides the unittest module.  It gives
    us the infrastructure for writing well-structured tests.</p>

    <p>The design of unittest is modelled on the common xUnit pattern that is
    available in many different languages, notably jUnit for Java.  This gives
    unittest a more verbose feeling than many Python libraries.  Some people
    don't like this and prefer other styles of tests, but unittest is by far
    the most widely used library, and is well-supported by every test tool.</p>

    <p>There are alternatives to unittest, but it's a good foundation to build
    on.</p>

</div><!-- }}} -->

<div class="hidden_slide">
    <h1>How NOT to do it</h1>
    <!--[[[cog
    include_code("""\
        class MyBadTestCase(unittest.TestCase):
            def test_a_thing(self):
                old_global = SOME_GLOBAL_THING
                SOME_GLOBAL_THING = new_test_value

                do_my_test_stuff()

                SOME_GLOBAL_THING = old_global
    """, lang="python")
    ]]]-->
    <pre class='python'>
    class MyBadTestCase(unittest.TestCase):
        def test_a_thing(self):
            old_global = SOME_GLOBAL_THING
            SOME_GLOBAL_THING = new_test_value

            do_my_test_stuff()

            SOME_GLOBAL_THING = old_global
    </pre>
    <!--[[[end]]]-->
    <ul>
        <li>Common impulse</li>
        <li>But if test fails, globals aren't restored</li>
    </ul>
</div>

<div class="hidden_slide">
    <h1>The right way</h1>
    <!--[[[cog
    include_code("""\
        class MyGoodTestCase(unittest.TestCase):
            def setUp(self):
                self.old_global = SOME_GLOBAL_THING
                SOME_GLOBAL_THING = new_test_value

            def tearDown(self):
                SOME_GLOBAL_THING = self.old_global

            def test_a_thing(self):
                do_my_test_stuff()
    """, lang="python")
    ]]]-->
    <pre class='python'>
    class MyGoodTestCase(unittest.TestCase):
        def setUp(self):
            self.old_global = SOME_GLOBAL_THING
            SOME_GLOBAL_THING = new_test_value

        def tearDown(self):
            SOME_GLOBAL_THING = self.old_global

        def test_a_thing(self):
            do_my_test_stuff()
    </pre>
    <!--[[[end]]]-->
    <ul>
        <li>tearDown gets run no matter what *</li>
        <li>Reuse: same setUp and tearDown apply to all the test methods</li>
        <li>Test method is small and focused</li>
        <li>* unless setUp fails</li>
    </ul>
</div>

<!-- }}} -->

<!-- *** Approaches {{{ -->
<div class="hidden_slide section" data-layout="clean">
    <h1>Approaches</h1>
    <h2>crafting tests</h2>
</div>

<div class="hidden_slide">
    <h1>What to test?</h1>
    <ul>
        <li>Any code that you want to work</li>
        <li>Start with what worries you</li>
        <li>Where are your bugs?</li>
    </ul>
</div>

<!--

- White box testing
- Black box testing
- Regression testing

- non-functional testing
    - performance
    - security
    - usability

-->

<div class="hidden_slide">
    <h1>How to start?</h1>
    <ul>
        <li>Pick a chunk of code</li>
        <li>What should it do?</li>
        <li>Test that</li>
        <li>What shouldn't it do?</li>
        <li>Test that</li>
    </ul>
</div>

<div class="hidden_slide">
    <h1>Test granularity</h1>
    <ul>
        <li>How big is the code chunk?</li>
        <li>Creates a hierarchy of tests:
            <ul>
                <li>Unit</li>
                <li>Functional</li>
                <li>Integration</li>
                <li>System</li>
            </ul></li>
        <li>Start small</li>
    </ul>
</div>


<div class="hidden_slide">
    <h1>What should the code do?</h1>
    <ul>
        <li>This can be difficult!</li>
        <li>Tests force you to decide</li>
        <li>Be as clear as you can</li>
        <li>(Document it in your docstrings)</li>
    </ul>
</div>

<div class="hidden_slide">
    <h1>Test what the code should do</h1>
    <ul>
        <li>Typical values</li>
        <li>Edge values</li>
        <li>Combinations of values</li>
    </ul>
</div>

<div class="hidden_slide">
    <h1>Test what the code shouldn't do</h1>
    <ul>
        <li>Error conditions</li>
        <li>Absurd values</li>
        <li>Common mistakes in usage</li>
        <li>Does it fail the way it should?</li>
    </ul>
</div>

<div class="hidden_slide">
    <h1>Where to put them?</h1>
    <ul>
        <li>Put tests in their own files</li>
        <li>Name them test_*.py</li>
        <li>Parallel to the code they're testing</li>
        <li>Pick your granularity</li>
    </ul>
</div>

<div class="hidden_slide">
    <h1>Good tests should be...</h1>
    <ul>
        <li>Automated</li>
        <li>Repeatable</li>
        <li>Convenient: fast</li>
        <li>Unambiguous</li>
    </ul>
</div>

<div class="hidden_slide">
    <h1>How much is enough?</h1>
    <ul>
        <li>Hard to know</li>
        <li>Depends on your needs</li>
        <li>Coverage measurement can help</li>
    </ul>
</div>

<div class="hidden_slide">
    <h1>Tests will fail</h1>
    <ul>
        <li>Could be the code is broken</li>
        <li>Could also be the tests are broken</li>
        <li>Tests have to be maintained</li>
        <li>Refactoring code might mean refactoring tests too</li>
    </ul>
</div>

<div class="hidden_slide">
    <h1>Test-Driven Development</h1>
    <ul>
        <li>Write tests before code!</li>
        <li>Ensures code is testable</li>
        <li>Produces better code</li>
    </ul>
</div>

<div class="hidden_slide">
    <h1>Testability</h1>
    <ul>
        <li>The quality of being testable</li>
        <li>Affects the design of your code</li>
        <li>Mostly: improved modularity</li>
        <li>See my PyCon 2010 talk: <a href='http://pycon.blip.tv/file/3261272'>Tests &amp; Testability</a></li>
    </ul>
</div>

<!-- }}} -->

<!-- *** nose {{{ -->
<div class="hidden_slide section" data-layout="clean">
    <h1>nose and pytest</h1>
    <h2>Running tests</h2>
</div>

<div class="hidden_slide">
    <h1>Test runners</h1>
    <ul>
        <li>Third-party test runners: nose, pytest, trial</li>
        <li>Know how to run your unittest tests</li>
        <li>Test discovery</li>
        <li>Run tests better</li>
        <li>Plugins provide lots of features</li>
    </ul>
</div>

<div class="hidden_slide">
    <h1>Test discovery</h1>
    <ul>
        <li>Running one file of tests is easy</li>
        <li>Pre-2.7 unittest: hard to run many test files</li>
        <li>New test runners provide test discovery</li>
        <li>They sniff out your tests wherever they are</li>
    </ul>
</div>

<!-- }}} -->

<!-- *** Other test runners {{{ -->
<div class="hidden_slide">
    <h1>Running tests</h1>
    <ul>
        <li>Show test names as they run</li>
        <li>Capture stdout</li>
        <li>Stop on first failure</li>
        <li>These are also in 2.7 unittest</li>
    </ul>
</div>

<div class="hidden_slide">
    <h1>Plugins</h1>
    <ul>
        <li>LOTS of useful functions added by plugins</li>
        <li>parallel execution</li>
        <li>attribute-based test selection</li>
        <li>output formats: color, subunit, xml, etc</li>
        <li>extreme isolation</li>
        <li>django integration</li>
        <li>etc</li>
    </ul>
</div>

<div class="hidden_slide">
    <h1>Class-less tests</h1>
    <ul>
        <li>*Unit patterns are based on Smalltalk</li>
        <li>But Python can have code outside of classes</li>
        <li>So why do my tests have to be in classes?</li>
        <li>Test can be just functions</li>
        <li>And why can't my asserts be plain asserts?</li>
    </ul>
</div>

<div class="hidden_slide">
    <h1>Test generators</h1>
    <ul>
        <li>Tests can be created with a generator</li>
        <li>Generator yields callable + args</li>
        <li>Each yield is a separate test</li>
        <li>This produces four tests, two pass:</li>
    </ul>
    <pre class='brush: python'>
    def test_evens():
        for i in range(4):
            yield check_even, i

    def check_even(n):
        assert n % 2 == 0
    </pre>
    <pre class='brush: python'>
    def test_0():
        check_even(0)
    def test_1():
        check_even(1)
    def test_2():
        check_even(2)
    def test_3():
        check_even(3)
    </pre>
</div>

<div class="hidden_slide">
    <h1>nose and unittest together</h1>
    <ul>
        <li>nose extended unittest</li>
        <li>Then unittest grew in 2.7</li>
        <li>They don't always get along</li>
        <li>Be careful</li>
    </ul>
</div>

<div class="hidden_slide highlight_code_red">
    <h1>pytest</h1>
    <pre class='brush: plain, highlight: [17,18,19]'>
    $ pytest test_port2_broken.py
    =========================== test session starts ===========================
    platform win32 - - Python 2.7.2 - - pytest-2.1.0
    collected 3 items

    test_port2_broken.py .F.

    ================================ FAILURES =================================
    _________________________ PortfolioTest.test_ibm __________________________

    self = &lt;test_port2_broken.PortfolioTest testMethod=test_ibm>

        def test_ibm(self):
            p = Portfolio()
            p.buy("IBM", 100, 176)
    &gt;       assert p.cost() == 17648.0
    E       AssertionError: assert 17600.0 == 17648.0
    E        +  where 17600.0 = &lt;bound method Portfolio.cost of &lt;portfolio1.Por
    E        +    where &lt;bound method Portfolio.cost of &lt;portfolio1.Portfolio o

    test_port2_broken.py:14: AssertionError
    =================== 1 failed, 2 passed in 0.12 seconds ====================
    </pre>
</div>

<div class="hidden_slide">
    <h1>Twisted's trial</h1>
    <ul>
        <li>Many advanced options</li>
        <li>Some particular to Twisted</li>
        <li>Control GC between tests</li>
        <li>Check global state</li>
    </ul>
</div>

<div class="hidden_slide">
    <h1>IDEs</h1>
    <ul>
        <li>IDEs like Eclipse and Netbeans also have test runners</li>
        <li>Vim and Emacs too</li>
        <li>I know nothing about them</li>
        <li>They are probably awesome</li>
    </ul>
</div>

<div class="hidden_slide">
    <h1>Test runners</h1>
    <ul>
        <li>unittest (2.7)</li>
        <li>nose, pytest, trial</li>
        <li>IDEs, Vim, Emacs</li>
    </ul>
</div>

<div class="hidden_slide">
    <h1>Lots of options</h1>
    <ul>
        <li>However you want to run your tests, you can</li>
        <li>Choose the runner that suits you</li>
        <li>Plugins allow further customization</li>
    </ul>
</div>

<!-- }}} -->

<!-- *** Coverage {{{ -->
<div class="hidden_slide section" data-layout="clean">
    <h1>Coverage</h1>
    <h2>Testing tests</h2>
</div>

<div class="hidden_slide">
    <h1>What code are you testing?</h1>
    <ul>
        <li>The goal: tests execute product code</li>
        <li>But do they really?</li>
        <li>How much of it?</li>
    </ul>
</div>

<div class="hidden_slide">
    <h1>Coverage measurement</h1>
    <ul>
        <li>Run your tests</li>
        <li>Track what parts of product code are executed</li>
        <li>Report on covered / not covered</li>
        <li>You find code not being tested</li>
        <li>Write more tests</li>
        <li>The world is better!</li>
    </ul>
</div>

<!--
<div class="hidden_slide">
    <h1>Running coverage.py</h1>
    <ul>
        <li>Install it from PyPI</li>
        <li><tt>coverage run my_prog.py arg ...</tt></li>
        <li><tt>coverage html</tt></li>
        <li>Or: &nbsp;<tt>nosetests - -with-cover ...</tt></li>
        <li>Or: &nbsp;<tt>pytest - -cov ...</tt></li>
    </ul>
</div>
-->

<!--
<div class="hidden_slide">
    <h1>HTML report</h1>
    <iframe src='test0_static/htmlcov/index.html' style='background:white;width:100%;height:85%;'></iframe>
</div>
-->

<div class="hidden_slide">
    <h1>Coverage can only tell you a few things</h1>
    <ul>
        <li>What lines were executed</li>
        <li>What branches were taken</li>
        <li>100% coverage is difficult to reach</li>
        <li>100% coverage doesn't tell you everything</li>
    </ul>
</div>

<div class="hidden_slide">
    <h1>What coverage can't tell you</h1>
    <ul>
        <li>Are you exercising all your data?</li>
        <li>Are you exercising all your HTML templates?</li>
        <li>Are you checking results properly?</li>
        <li>Are you hitting all edge conditions?</li>
        <li>Are you testing the right things?</li>
        <li>Are you building the right product?  ;-)</li>
    </ul>
</div>


<!-- }}} -->

<!-- *** Using unittest {{{ -->

<div class="slide section" data-layout="clean">
    <h1>unittest</h1>
    <h2>Writing tests</h2>
</div>

<div class="slide">
    <h1>unittest</h1>
    <ul>
        <li>Python standard library</li>
        <li>Infrastructure for well-structured tests</li>
        <li>Patterned on xUnit</li>
    </ul>
</div>

<div class="slide">
    <h1>A simple unit test</h1>
    <!--[[[cog include_file("test_port1.py")   ]]] {{{ -->
    <pre class='python medium'>
    # test_port1.py

    import unittest
    from portfolio1 import Portfolio

    class PortfolioTest(unittest.TestCase):
        def test_buy_one_stock(self):
            p = Portfolio()
            p.buy("IBM", 100, 176.48)
            assert p.cost() == 17648.0
    </pre>
    <!--[[[end]]] }}}-->
    <!--[[[cog include_file("test_port1.out")   ]]] {{{ -->
    <pre class='text medium'>
    $ python -m unittest test_port1
    .
    ------------------------------------------------------------
    Ran 1 test in 0.000s

    OK
    </pre>
    <!--[[[end]]] }}}-->
</div>

<div class="text"><!-- {{{ -->

    <p>Here is an actual unit test written with unittest:</p>

<!--[[[cog include_file("test_port1.py", px=True) ]]] {{{ -->
<code lang='python'>
# test_port1.py

import unittest
from portfolio1 import Portfolio

class PortfolioTest(unittest.TestCase):
    def test_buy_one_stock(self):
        p = Portfolio()
        p.buy("IBM", 100, 176.48)
        assert p.cost() == 17648.0
</code>
<!--[[[end]]] }}}-->

    <p>Let's look at the structure of the test in detail.</p>

    <p>All tests are written as methods in a new class.  The class is derived
    from unittest.TestCase, and usually has a name with the word Test in it.</p>

    <p>The test methods all must start with the word "test_".  Here we have
    only one, called "test_buy_one_stock".  The body of the method is the test
    itself.  We create a Portfolio, buy 100 shares of IBM, and then make an
    assertion about the resulting cost.</p>

    <p>This is a complete test.  To run it, we use the unittest module as a
    test runner.  "python -m unittest" means, instead of running a Python
    program in a file, run the importable module "unittest" as the main
    program.</p>

    <p>The unittest module accepts a number of forms of command-line arguments
    telling it what to run.  In this case we give it the module name
    "test_port1".  It will import that module, find the test classes and
    methods in it, and run them all.</p>

    <p>We can see from the output that it ran 1 test, and it passed:</p>

<!--[[[cog include_file("test_port1.out", px=True) ]]] {{{ -->
<code lang='text'>
$ python -m unittest test_port1
.
------------------------------------------------------------
Ran 1 test in 0.000s

OK
</code>
<!--[[[end]]] }}}-->

</div><!-- }}} -->

<div class="slide">
    <h1>Under the hood</h1>
    <!--[[[cog
    include_code("""\
        # unittest runs the tests as if I had written:
        testcase = PortfolioTest()
        try:
            testcase.test_buy_one_stock()
        except AssertionError:
            [record failure]
        else:
            [record success]
    """, lang="python")
    ]]]-->
    <pre class='python'>
    # unittest runs the tests as if I had written:
    testcase = PortfolioTest()
    try:
        testcase.test_buy_one_stock()
    except AssertionError:
        [record failure]
    else:
        [record success]
    </pre>
    <!--[[[end]]]-->
</div>

<div class="text"><!-- {{{ -->
    <p>Our test code is organized a bit differently that you might have
    expected.  To help understand why, this is what the test runner does to
    execute your tests.</p>

    <p>To run a test, unittest creates an instance of your test case class.
    Then it executes the test method on that object.  It wraps the execution
    in a try/except clause so that assertion failures will be recorded as test
    failures, and if no exception happens, it's recorded as a succcess.</p>

</div><!-- }}} -->

<div class="slide" data-layout="clean">
    <h1>Add more tests</h1>
    <!--[[[cog include_file("test_port2.py", start=6, end=20)   ]]] {{{ -->
    <pre class='python medium'>
    class PortfolioTest(unittest.TestCase):
        def test_empty(self):
            p = Portfolio()
            assert p.cost() == 0.0

        def test_buy_one_stock(self):
            p = Portfolio()
            p.buy("IBM", 100, 176.48)
            assert p.cost() == 17648.0

        def test_buy_two_stocks(self):
            p = Portfolio()
            p.buy("IBM", 100, 176.48)
            p.buy("HPQ", 100, 36.15)
            assert p.cost() == 21263.0
    </pre>
    <!--[[[end]]] }}}-->
    <!--[[[cog include_file("test_port2.out", hilite=["..."])   ]]] {{{ -->
    <pre class='text medium' data-hilite='|1|'>
    $ python -m unittest test_port2
    ...
    ------------------------------------------------------------
    Ran 3 tests in 0.000s

    OK
    </pre>
    <!--[[[end]]] }}}-->
    <ul>
        <li>A dot for every passed test</li>
    </ul>
</div>

<div class="text"><!-- {{{ -->

    <p>One test isn't enough, let's add some more.  Here we add a simpler test,
    test_empty, and a more complicated test, test_buy_two_stocks.  Each test is
    another test method in our PortfolioTest class:</p>

<!--[[[cog include_file("test_port2.py", px=True) ]]] {{{ -->
<code lang='python'>
# test_port2.py

import unittest
from portfolio1 import Portfolio

class PortfolioTest(unittest.TestCase):
    def test_empty(self):
        p = Portfolio()
        assert p.cost() == 0.0

    def test_buy_one_stock(self):
        p = Portfolio()
        p.buy("IBM", 100, 176.48)
        assert p.cost() == 17648.0

    def test_buy_two_stocks(self):
        p = Portfolio()
        p.buy("IBM", 100, 176.48)
        p.buy("HPQ", 100, 36.15)
        assert p.cost() == 21263.0
</code>
<!--[[[end]]] }}}-->

    <p>Each one creates the portfolio object it needs, performs the
    manipulations it wants, and makes assertions about the outcome.</p>

    <p>When you run the tests, it prints a dot for every test that passes,
    which is why you see "..." in the test output here:</p>

<!--[[[cog include_file("test_port2.out", px=True) ]]] {{{ -->
<code lang='text'>
$ python -m unittest test_port2
...
------------------------------------------------------------
Ran 3 tests in 0.000s

OK
</code>
<!--[[[end]]] }}}-->

</div><!-- }}} -->

<div class="slide" data-layout="clean">
    <h1>Under the hood</h1>
    <!--[[[cog
    include_code("""\
        # unittest runs the tests as if I had written:
        testcase = PortfolioTest()
        try:
            testcase.test_empty()
        except AssertionError:
            [record failure]
        else:
            [record success]

        testcase = PortfolioTest()
        try:
            testcase.test_buy_one_stock()
        except AssertionError:
            [record failure]
        else:
            [record success]

        testcase = PortfolioTest()
        try:
            testcase.test_buy_two_stocks()
        except AssertionError:
            [record failure]
        else:
            [record success]
    """, lang="python", hilite=[3, 11, 19])
    ]]]-->
    <pre class='python' data-hilite='|3|11|19|'>
    # unittest runs the tests as if I had written:
    testcase = PortfolioTest()
    try:
        testcase.test_empty()
    except AssertionError:
        [record failure]
    else:
        [record success]

    testcase = PortfolioTest()
    try:
        testcase.test_buy_one_stock()
    except AssertionError:
        [record failure]
    else:
        [record success]

    testcase = PortfolioTest()
    try:
        testcase.test_buy_two_stocks()
    except AssertionError:
        [record failure]
    else:
        [record success]
    </pre>
    <!--[[[end]]]-->
</div>

<div class="text"><!-- {{{ -->
    <p>With three tests, the execution model is much as before.  The key
    thing to note here is that a new instance of PortfolioTest is created
    for each test method.  This helps to guarantee an important property of
    good tests: isolation.</p>

</div><!-- }}} -->

<div class="slide">
    <h1>Test isolation</h1>
    <ul>
        <li>Every test gets a new test object</li>
        <li>Tests can't affect each other</li>
        <li>Failure doesn't stop next tests</li>
    </ul>
</div>

<div class="text"><!-- {{{ -->

    <p>Test isolation means that each of your tests is unaffected by every
    other test. This is good because it makes your tests more repeatable, and
    they are clearer about what they are testing. It also means that if a
    test fails, you don't have to think about all the conditions and data
    created by earlier tests: running just that one test will reproduce the
    failure.</p>

    <p>Earlier we had a problem where one test failing prevented the other
    tests from running.  Here unittest is running each test independently, so
    if one fails, the rest will run, and will run just as if the earlier test
    had succeeded.</p>

</div><!-- }}} -->

<div class="slide">
    <h1>What failure looks like</h1>
    <!--[[[cog include_file("test_port2_broken.out")   ]]] {{{ -->
    <pre class='text medium'>
    $ python -m unittest test_port2_broken
    F..
    ============================================================
    FAIL: test_buy_one_stock (test_port2_broken.PortfolioTest)
    ------------------------------------------------------------
    Traceback (most recent call last):
      File "/Users/ned/prz/test0/test_port2_broken.py", line 14, in test_buy_one_stock
        assert p.cost() == 17648.0
    AssertionError

    ------------------------------------------------------------
    Ran 3 tests in 0.001s

    FAILED (failures=1)
    </pre>
    <!--[[[end]]] }}}-->
    <ul>
        <li class="good">Better: failed test didn't stop others</li>
        <li class="bad">Bad: what value was returned?</li>
    </ul>
</div>

<div class="text"><!-- {{{ -->

    <p>So far, all of our tests have passed.  What happens when they fail?</p>

<!--[[[cog include_file("test_port2_broken.out", px=True) ]]] {{{ -->
<code lang='text'>
$ python -m unittest test_port2_broken
F..
============================================================
FAIL: test_buy_one_stock (test_port2_broken.PortfolioTest)
------------------------------------------------------------
Traceback (most recent call last):
  File "/Users/ned/prz/test0/test_port2_broken.py", line 14, in test_buy_one_stock
    assert p.cost() == 17648.0
AssertionError

------------------------------------------------------------
Ran 3 tests in 0.001s

FAILED (failures=1)
</code>
<!--[[[end]]] }}}-->

    <p>The test runner prints a dot for every test that passes, and it prints
    an "F" for each test failure, so here we see ".F." in the output.  Then
    for each test failure, it prints the name of the test, and the traceback
    of the assertion failure.</p>

    <p>This style of test output means that test successes are very quiet, just
    a single dot.  When a test fails, it stands out, and you can focus on them.
    Remember: when your tests pass, you don't have to do anything, you can go
    on with other work, so passing tests, while a good thing, should not cause
    a lot of noise.  It's the failing tests we need to think about.</p>

    <p>It's great that the traceback shows what assertion failed, but notice
    that it doesn't tell us what bad value was returned.  We can see that we
    expected it to be 17648.0, but we don't know what the actual value was.</p>

</div><!-- }}} -->

<div class="slide">
    <h1>unittest assert helpers</h1>
    <p>self.assertEqual(x, y) instead of assert x == y</p>
    <!--[[[cog include_file("test_port3.py", start=11, end=14, highlight=[14])   ]]] {{{ -->
    <pre class='python medium'>
    def test_buy_one_stock(self):
        p = Portfolio()
        p.buy("IBM", 100, 176.48)
        self.assertEqual(p.cost(), 17648.0)
    </pre>
    <!--[[[end]]] }}}-->
    <!--[[[cog include_file("test_port3_broken.out", highlight=[9])   ]]] {{{ -->
    <pre class='text medium'>
    $ python -m unittest test_port3_broken
    F..
    ============================================================
    FAIL: test_buy_one_stock (test_port3_broken.PortfolioTest)
    ------------------------------------------------------------
    Traceback (most recent call last):
      File "/Users/ned/prz/test0/test_port3_broken.py", line 14, in test_buy_one_stock
        self.assertEqual(p.cost(), 17648.0)
    AssertionError: 17600.0 != 17648.0

    ------------------------------------------------------------
    Ran 3 tests in 0.001s

    FAILED (failures=1)
    </pre>
    <!--[[[end]]] }}}-->
</div>

<div class="text"><!-- {{{ -->

    <p>One of the reasons to derive your test classes from unittest.TestCase
    is that it provides you with specialized assertion methods.  Here we
    rewrite our assertion using one of those methods:</p>

<!--[[[cog include_file("test_port3.py", px=True, start=11, end=14) ]]] {{{ -->
<code lang='python'>
def test_buy_one_stock(self):
    p = Portfolio()
    p.buy("IBM", 100, 176.48)
    self.assertEqual(p.cost(), 17648.0)
</code>
<!--[[[end]]] }}}-->

    <p>In this test, we've used self.assertEqual instead of the built-in
    assert statement.  The benefit of the method is that it can print both the
    actual and expected values if the test fails.</p>

    <p>Here the assertion error message shows both values.  The best tests
    will give you information you can use to diagnose and debug the problem.
    Here, when comparing two values for equality, we are told not only that
    they are not equal, but also what each value was.</p>

</div><!-- }}} -->

<div class="slide">
    <h1>Lots of assert helpers</h1>
    <!--[[[cog
    include_code("""\
        assertEqual(first, second)
        assertNotEqual(first, second)
        assertTrue(expr)
        assertFalse(expr)
        assertIn(first, second)
        assertNotIn(first, second)
        assertIs(first, second)
        assertIsNot(first, second)
        assertAlmostEqual(first, second)
        assertNotAlmostEqual(first, second)
        assertGreater(first, second)
        assertLess(first, second)
        assertRegexpMatches(text, regexp)
        assertRaises(exc_class, func, ...)
        assertSequenceEqual(seq1, seq2)
        assertItemsEqual(seq1, seq2)

        .. etc ..
    """, lang="python")
    ]]]-->
    <pre class='python'>
    assertEqual(first, second)
    assertNotEqual(first, second)
    assertTrue(expr)
    assertFalse(expr)
    assertIn(first, second)
    assertNotIn(first, second)
    assertIs(first, second)
    assertIsNot(first, second)
    assertAlmostEqual(first, second)
    assertNotAlmostEqual(first, second)
    assertGreater(first, second)
    assertLess(first, second)
    assertRegexpMatches(text, regexp)
    assertRaises(exc_class, func, ...)
    assertSequenceEqual(seq1, seq2)
    assertItemsEqual(seq1, seq2)

    .. etc ..
    </pre>
    <!--[[[end]]]-->
</div>

<div class="text"><!-- {{{ -->

    <p>The unittest.TestCase class provides many assertion methods for testing
    many conditions.  Read through the
    <a href='http://docs.python.org/2/library/unittest.html'>unittest documentation</a>
    to familiarize yourself with the variety available.</p>

</div><!-- }}} -->


<div class="slide">
    <h1>Pro tip: your own base class</h1>
    <!--[[[cog include_file("test_port3b.py", start=6)      ]]] {{{ -->
    <pre class='python medium'>
    class PortfolioTestCase(unittest.TestCase):
        """Base class for all Portfolio tests."""

        def assertCostEqual(self, p, cost):
            """Assert that `p`'s cost is equal to `cost`."""
            self.assertEqual(p.cost(), cost)


    class PortfolioTest(PortfolioTestCase):
        def test_empty(self):
            p = Portfolio()
            self.assertCostEqual(p, 0.0)

        def test_buy_one_stock(self):
            p = Portfolio()
            p.buy("IBM", 100, 176.48)
            self.assertCostEqual(p, 17648.0)

        def test_buy_two_stocks(self):
            p = Portfolio()
            p.buy("IBM", 100, 176.48)
            p.buy("HPQ", 100, 36.15)
            self.assertCostEqual(p, 21263.0)
    </pre>
    <!--[[[end]]] }}}-->
</div>

<div class="text"><!-- {{{ -->

    <p>One simple thing you can do that will help as your test code grows,
    is to define your own base class for your test classes.  You derive a new
    test case class from unittest.TestCase, and then derive all your test
    classes from that class.</p>

    <p>This gives you a place that you can add new assert methods, or other
    test helpers.  Here we've defined an assertCostEqual method that is
    specialized for our portfolio testing:</p>

<!--[[[cog include_file("test_port3b.py", px=True, start=6) ]]] {{{ -->
<code lang='python'>
class PortfolioTestCase(unittest.TestCase):
    """Base class for all Portfolio tests."""

    def assertCostEqual(self, p, cost):
        """Assert that `p`'s cost is equal to `cost`."""
        self.assertEqual(p.cost(), cost)


class PortfolioTest(PortfolioTestCase):
    def test_empty(self):
        p = Portfolio()
        self.assertCostEqual(p, 0.0)

    def test_buy_one_stock(self):
        p = Portfolio()
        p.buy("IBM", 100, 176.48)
        self.assertCostEqual(p, 17648.0)

    def test_buy_two_stocks(self):
        p = Portfolio()
        p.buy("IBM", 100, 176.48)
        p.buy("HPQ", 100, 36.15)
        self.assertCostEqual(p, 21263.0)
</code>
<!--[[[end]]] }}}-->

    <p>This class only has a single new method, and that method is pretty
    simple, but we're only testing a small product in this code.  Your real
    system will likely have plenty of opportunities to define domain-specific
    helpers that can make your tests more concise and more readable.</p>

</div><!-- }}} -->


<div class="slide">
    <h1>Third possible outcome: E</h1>
    <p>Test raises an exception</p>
    <!--[[[cog include_file("test_port3_broken2.out", highlight=[2,8,9])   ]]] {{{ -->
    <pre class='text medium'>
    $ python -m unittest test_port3_broken2
    E..
    ============================================================
    ERROR: test_buy_one_stock (test_port3_broken2.PortfolioTest)
    ------------------------------------------------------------
    Traceback (most recent call last):
      File "/Users/ned/prz/test0/test_port3_broken2.py", line 13, in test_buy_one_stock
        p.buyXX("IBM", 100, 176.48)
    AttributeError: 'Portfolio' object has no attribute 'buyXX'

    ------------------------------------------------------------
    Ran 3 tests in 0.000s

    FAILED (errors=1)
    </pre>
    <!--[[[end]]] }}}-->
</div>

<div class="text"><!-- {{{ -->

    <p>There are actually three possible outcomes for a test: passing, failure,
    and error.  Error means that the test raised an exception other than
    AssertionError.  Well-written tests should either succeed, or should fail
    in an assertion.  If another exception happens, it could either mean that
    the test is broken, or that the code is broken, but either way, it isn't
    what you intended.  This condition is marked with an "E", and the error
    stacktrace is displayed:</p>

<!--[[[cog include_file("test_port3_broken2.out", px=True) ]]] {{{ -->
<code lang='text'>
$ python -m unittest test_port3_broken2
E..
============================================================
ERROR: test_buy_one_stock (test_port3_broken2.PortfolioTest)
------------------------------------------------------------
Traceback (most recent call last):
  File "/Users/ned/prz/test0/test_port3_broken2.py", line 13, in test_buy_one_stock
    p.buyXX("IBM", 100, 176.48)
AttributeError: 'Portfolio' object has no attribute 'buyXX'

------------------------------------------------------------
Ran 3 tests in 0.000s

FAILED (errors=1)
</code>
<!--[[[end]]] }}}-->

    <p>In this case, the mistake is in the test: we tried to call a method
    that doesn't exist.</p>

</div><!-- }}} -->


<div class="hidden_slide">
    <h1>Under the hood</h1>
    <!--[[[cog
    include_code("""\
        testcase = PortfolioTest()
        try:
            testcase.test_method()
        except AssertionError:
            [record failure]
        except:
            [record error]
        else:
            [record success]
    """, lang="python")
    ]]]-->
    <pre class='python'>
    testcase = PortfolioTest()
    try:
        testcase.test_method()
    except AssertionError:
        [record failure]
    except:
        [record error]
    else:
        [record success]
    </pre>
    <!--[[[end]]]-->
</div>

<div class="slide">
    <h1>Can't call the function</h1>
    <!--[[[cog include_file("test_port4_broken.py", start=22, end=24)   ]]] {{{ -->
    <pre class='python medium'>
    def test_bad_input(self):
        p = Portfolio()
        p.buy("IBM")
    </pre>
    <!--[[[end]]] }}}-->
    <!--[[[cog include_file("test_port4_broken.out")   ]]] {{{ -->
    <pre class='text medium'>
    $ python -m unittest test_port4_broken
    E...
    ============================================================
    ERROR: test_bad_input (test_port4_broken.PortfolioTest)
    ------------------------------------------------------------
    Traceback (most recent call last):
      File "/Users/ned/prz/test0/test_port4_broken.py", line 24, in test_bad_input
        p.buy("IBM")
    TypeError: buy() missing 2 required positional arguments: 'shares' and 'price'

    ------------------------------------------------------------
    Ran 4 tests in 0.000s

    FAILED (errors=1)
    </pre>
    <!--[[[end]]] }}}-->
</div>

<div class="text"><!-- {{{ -->

    <p>Here we try to write an automated test of an error case: calling a
    method with too few arguments:</p>

<!--[[[cog include_file("test_port4_broken.py", start=22, end=24, px=True) ]]] {{{ -->
<code lang='python'>
def test_bad_input(self):
    p = Portfolio()
    p.buy("IBM")
</code>
<!--[[[end]]] }}}-->

    <p>This test won't do what we want.  When we call buy() with too few
    arguments, of course it raises TypeError, and there's nothing to catch the
    exception, so the test ends with an Error status:</p>

<!--[[[cog include_file("test_port4_broken.out", px=True) ]]] {{{ -->
<code lang='text'>
$ python -m unittest test_port4_broken
E...
============================================================
ERROR: test_bad_input (test_port4_broken.PortfolioTest)
------------------------------------------------------------
Traceback (most recent call last):
  File "/Users/ned/prz/test0/test_port4_broken.py", line 24, in test_bad_input
    p.buy("IBM")
TypeError: buy() missing 2 required positional arguments: 'shares' and 'price'

------------------------------------------------------------
Ran 4 tests in 0.000s

FAILED (errors=1)
</code>
<!--[[[end]]] }}}-->

    <p>That's not good, we want all our tests to pass.</p>

</div><!-- }}} -->

<div class="slide">
    <h1>assertRaises</h1>
    <!--[[[cog include_file("test_port4.py", start=22, end=25, highlight=[24,25])   ]]] {{{ -->
    <pre class='python medium'>
    def test_bad_input(self):
        p = Portfolio()
        with self.assertRaises(TypeError):
            p.buy("IBM")
    </pre>
    <!--[[[end]]] }}}-->
    <!--[[[cog include_file("test_port4.out")   ]]] {{{ -->
    <pre class='text medium'>
    $ python -m unittest test_port4
    ....
    ------------------------------------------------------------
    Ran 4 tests in 0.000s

    OK
    </pre>
    <!--[[[end]]] }}}-->
</div>

<div class="text"><!-- {{{ -->

    <p>To properly handle the error-raising function call, we use a method
    called assertRaises:</p>

<!--[[[cog include_file("test_port4.py", start=22, end=25, px=True) ]]] {{{ -->
<code lang='python'>
def test_bad_input(self):
    p = Portfolio()
    with self.assertRaises(TypeError):
        p.buy("IBM")
</code>
<!--[[[end]]] }}}-->

    <p>This neatly captures our intent: we are asserting that a statement will
    raise an exception.  It's used as a context manager with a "with" statement
    so that it can handle the exception when it is raised:</p>

<!--[[[cog include_file("test_port4.out", px=True) ]]] {{{ -->
<code lang='text'>
$ python -m unittest test_port4
....
------------------------------------------------------------
Ran 4 tests in 0.000s

OK
</code>
<!--[[[end]]] }}}-->

    <p>Now our test passes because the TypeError is caught by the assertRaises
    context manager.  The assertion passes because the exception raised is the
    same type we claimed it would be, and all is well.</p>

    <p>If you have experience with unittest before 2.7, the context manager
    style is new, and much more convenient than the old way.</p>

</div><!-- }}} -->

<div class="slide">
    <h1>Portfolio: .sell()</h1>
    <!--[[[cog include_file("portfolio2.py", start=21, end=30)   ]]] {{{ -->
    <pre class='python medium'>
    def sell(self, name, shares):
        """Sell some shares."""
        for holding in self.stocks:
            if holding[0] == name:
                if holding[1] &lt; shares:
                    raise ValueError("Not enough shares")
                holding[1] -= shares
                break
        else:
            raise ValueError("You don't own that stock")
    </pre>
    <!--[[[end]]] }}}-->
</div>

<div class="text"><!-- {{{ -->

    <p>Our testing is going well, time to extend our product.  Let's add
    a .sell() method to our Portfolio class.  It will remove shares of a
    particular stock from our Portfolio:</p>

<!--[[[cog include_file("portfolio2.py", start=21, end=30, px=True) ]]] {{{ -->
<code lang='python'>
def sell(self, name, shares):
    """Sell some shares."""
    for holding in self.stocks:
        if holding[0] == name:
            if holding[1] &lt; shares:
                raise ValueError("Not enough shares")
            holding[1] -= shares
            break
    else:
        raise ValueError("You don't own that stock")
</code>
<!--[[[end]]] }}}-->

    <p>Note: this code is very simple, for the purpose of fitting on a slide!</p>

</div><!-- }}} -->

<div class="slide" data-layout="clean">
    <h1>Testing sell()</h1>
    <!--[[[cog include_file("test_port5.py", start=36)   ]]] {{{ -->
    <pre class='python medium'>
    class PortfolioSellTest(PortfolioTestCase):
        def test_sell(self):
            p = Portfolio()
            p.buy("MSFT", 100, 27.0)
            p.buy("DELL", 100, 17.0)
            p.buy("ORCL", 100, 34.0)
            p.sell("MSFT", 50)
            self.assertCostEqual(p, 6450)

        def test_not_enough(self):
            p = Portfolio()             # Didn't I just do this?
            p.buy("MSFT", 100, 27.0)
            p.buy("DELL", 100, 17.0)
            p.buy("ORCL", 100, 34.0)
            with self.assertRaises(ValueError):
                p.sell("MSFT", 200)

        def test_dont_own_it(self):
            p = Portfolio()             # What, again!?!?
            p.buy("MSFT", 100, 27.0)
            p.buy("DELL", 100, 17.0)
            p.buy("ORCL", 100, 34.0)
            with self.assertRaises(ValueError):
                p.sell("IBM", 1)
    </pre>
    <!--[[[end]]] }}}-->
</div>

<div class="text"><!-- {{{ -->

    <p>To test the .sell() method, we add three more tests.  In each case,
    we need to create a portfolio with some stocks in it so that we have
    something to sell:</p>

<!--[[[cog include_file("test_port5.py", start=36, px=True) ]]] {{{ -->
<code lang='python'>
class PortfolioSellTest(PortfolioTestCase):
    def test_sell(self):
        p = Portfolio()
        p.buy("MSFT", 100, 27.0)
        p.buy("DELL", 100, 17.0)
        p.buy("ORCL", 100, 34.0)
        p.sell("MSFT", 50)
        self.assertCostEqual(p, 6450)

    def test_not_enough(self):
        p = Portfolio()             # Didn't I just do this?
        p.buy("MSFT", 100, 27.0)
        p.buy("DELL", 100, 17.0)
        p.buy("ORCL", 100, 34.0)
        with self.assertRaises(ValueError):
            p.sell("MSFT", 200)

    def test_dont_own_it(self):
        p = Portfolio()             # What, again!?!?
        p.buy("MSFT", 100, 27.0)
        p.buy("DELL", 100, 17.0)
        p.buy("ORCL", 100, 34.0)
        with self.assertRaises(ValueError):
            p.sell("IBM", 1)
</code>
<!--[[[end]]] }}}-->

    <p>But now our tests are getting really repetitve.  We've used the same
    four lines of code to create the same portfolio object three times.</p>

</div><!-- }}} -->


<div class="slide">
    <h1>Setting up a test</h1>
    <!--[[[cog include_file("test_port6.py", start=36)   ]]] {{{ -->
    <pre class='python medium'>
    class PortfolioSellTest(PortfolioTestCase):
        # Invoked before each test method
        def setUp(self):
            self.p = Portfolio()
            self.p.buy("MSFT", 100, 27.0)
            self.p.buy("DELL", 100, 17.0)
            self.p.buy("ORCL", 100, 34.0)

        def test_sell(self):
            self.p.sell("MSFT", 50)
            self.assertCostEqual(self.p, 6450)

        def test_not_enough(self):
            with self.assertRaises(ValueError):
                self.p.sell("MSFT", 200)

        def test_dont_own_it(self):
            with self.assertRaises(ValueError):
                self.p.sell("IBM", 1)
    </pre>
    <!--[[[end]]] }}}-->
</div>

<div class="text"><!-- {{{ -->

    <p>This is a common problem, so unittest has a solution for us.  A test
    class can define a .setUp() method.  This method is invoked before each
    test method.  Because your test class is a class, you have a self object
    that you can create attributes on:</p>

<!--[[[cog include_file("test_port6.py", start=36, px=True) ]]] {{{ -->
<code lang='python'>
class PortfolioSellTest(PortfolioTestCase):
    # Invoked before each test method
    def setUp(self):
        self.p = Portfolio()
        self.p.buy("MSFT", 100, 27.0)
        self.p.buy("DELL", 100, 17.0)
        self.p.buy("ORCL", 100, 34.0)

    def test_sell(self):
        self.p.sell("MSFT", 50)
        self.assertCostEqual(self.p, 6450)

    def test_not_enough(self):
        with self.assertRaises(ValueError):
            self.p.sell("MSFT", 200)

    def test_dont_own_it(self):
        with self.assertRaises(ValueError):
            self.p.sell("IBM", 1)
</code>
<!--[[[end]]] }}}-->

    <p>Here we have a .setup() method that creates a portfolio and stores it as
    self.p.  Then the three test methods can simply use the self.p portfolio.
    Note the three test methods are much smaller, since they share the common
    setup code in .setUp().</p>

    <p>Naturally, there's also a .tearDown() method that will be invoked when
    the test method is finished.  tearDown can clean up after a test, for
    example if your setUp created records in a database, or files on disk.  We
    don't need one here.</p>

</div><!-- }}} -->

<div class="slide">
    <h1>Under the hood</h1>
    <!--[[[cog
    include_code("""\
        testcase = PortfolioTest()
        try:
            testcase.setUp()
        except:
            [record error]
        else:
            try:
                testcase.test_method()
            except AssertionError:
                [record failure]
            except:
                [record error]
            else:
                [record success]
            finally:
                try:
                    testcase.tearDown()
                except:
                    [record error]
    """, lang="python", highlight=[3,17])
    ]]]-->
    <pre class='python'>
    testcase = PortfolioTest()
    try:
        testcase.setUp()
    except:
        [record error]
    else:
        try:
            testcase.test_method()
        except AssertionError:
            [record failure]
        except:
            [record error]
        else:
            [record success]
        finally:
            try:
                testcase.tearDown()
            except:
                [record error]
    </pre>
    <!--[[[end]]]-->
</div>

<div class="text"><!-- {{{ -->

    <p>Here's the detail on how unittest runs the setUp, test_method, and
    tearDown methods.  A test case object is instantiated, and its .setUp() is
    run.  if it doesn't raise an exception, then the .test_method() is run,
    noting its outcome.  No matter what happens with the .test_method(), the
    tearDown() is run.</p>

    <p>This is an important reason to put clean up code in a tearDown() method:
    it will be run even if your test fails.  If you try to clean up after
    yourself at the end of your test method, then if the test fails, the
    clean up doesn't happen, and you may pollute other tests.</p>

</div><!-- }}} -->

<div class="slide">
    <h1>setUp and tearDown: isolation!</h1>
    <ul>
        <li>Establish context</li>
        <li>Common pre- or post- work</li>
        <li>Isolation, even with failures</li>
        <li>Also: "fixtures"</li>
    </ul>
</div>

<div class="text"><!-- {{{ -->

    <p>setUp and tearDown are important ways to write more concise tests, and
    more importantly, give you powerful tools for ensuring proper test
    isolation.</p>

    <p>If you find your setUp and tearDown methods getting elaborate, there are
    third party "fixture" libraries that can help automate the creation of
    test data and environments.</p>

</div><!-- }}} -->

<div class="hidden_slide">
    <h1>How NOT to do it</h1>
    <!--[[[cog
    include_code("""\
        class MyBadTestCase(unittest.TestCase):
            def test_a_thing(self):
                old_global = SOME_GLOBAL_THING
                SOME_GLOBAL_THING = new_test_value

                do_my_test_stuff()

                SOME_GLOBAL_THING = old_global
    """, lang="python")
    ]]]-->
    <pre class='python'>
    class MyBadTestCase(unittest.TestCase):
        def test_a_thing(self):
            old_global = SOME_GLOBAL_THING
            SOME_GLOBAL_THING = new_test_value

            do_my_test_stuff()

            SOME_GLOBAL_THING = old_global
    </pre>
    <!--[[[end]]]-->
    <ul>
        <li>Common impulse</li>
        <li>But if test fails, globals aren't restored</li>
    </ul>
</div>

<div class="hidden_slide">
    <h1>The right way</h1>
    <!--[[[cog
    include_code("""\
        class MyGoodTestCase(unittest.TestCase):
            def setUp(self):
                self.old_global = SOME_GLOBAL_THING
                SOME_GLOBAL_THING = new_test_value

            def tearDown(self):
                SOME_GLOBAL_THING = self.old_global

            def test_a_thing(self):
                do_my_test_stuff()
    """, lang="python")
    ]]]-->
    <pre class='python'>
    class MyGoodTestCase(unittest.TestCase):
        def setUp(self):
            self.old_global = SOME_GLOBAL_THING
            SOME_GLOBAL_THING = new_test_value

        def tearDown(self):
            SOME_GLOBAL_THING = self.old_global

        def test_a_thing(self):
            do_my_test_stuff()
    </pre>
    <!--[[[end]]]-->
    <ul>
        <li>tearDown gets run no matter what *</li>
        <li>Reuse: same setUp and tearDown apply to all the test methods</li>
        <li>Test method is small and focused</li>
        <li>* unless setUp fails</li>
    </ul>
</div>

<!-- }}} -->

</body>
</html>
<!-- TO DO:  {{{

    * Content
    - Something to replace the hidden Approaches section
    - Most common question: how do I convince my team to test?

    * Layout
    - URL footer on first few slides.



    # Getting Started Testing

    - Goal
        - Give you tools to start testing your code

    - Two challenges:
        - People
        - Code

    - Vegetables

    = People

    - Why test?
        - Best way to know if your code works
    - Resistance
        - More work
        - Hard to add to a legacy project
        - Not a developer's job?
    - Benefits
        - Confidence
        - Turns fear into boredom
        - Faster in the long run
    - Fighting Chaos!
    - How to start
        - Start with the easy
        - Start with the necessary

    = Code

    - 2 vs 3

    - How to write tests
        - Show *a* way to do it.
        - there are many techniques

    = First principles

    - Product code: stock portfolio

    - First test: interactive

    - Second test: standalone

    - Third test: with expected results

    - Fourth test: check results automatically

    - Getting complicated!

    = Unittest

    - unittest
        - stdlib
        - a good solid foundation
        - not the only way to do it
        - not about unit vs integration

    - Simple unit test

    = Test Doubles


    = Other stuff
    - doctest
    - coverage
    - nose, pytest, trial
    - lettuce
    - Selenium
    - ddt
    - TDD

}}} -->
